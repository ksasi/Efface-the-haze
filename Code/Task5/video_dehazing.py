# -*- coding: utf-8 -*-
"""Video_Dehazing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7Xepjihku_9B_qy5M0aOJVt09ZL7Jnu
"""

!nvidia-smi

!pip install git+https://github.com/ChristophReich1996/Involution

# Import necessary libraries

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split, SubsetRandomSampler
from torchvision.datasets import CIFAR10
from torchvision import datasets, transforms
from torch.optim import *

import os
import random
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math
import cv2
import glob
import copy

from sklearn.model_selection import train_test_split
#import wandb

from torchsummary import summary

from skimage.feature import hog
from tqdm import tqdm as tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

seed = 42

from numba import jit, cuda

import torch
import numpy as np
from torch.utils.data import TensorDataset, DataLoader, Dataset

from PIL import Image
import glob

import albumentations as A
from albumentations.pytorch import ToTensor


import torch
from involution import Involution2d

#involution = Involution2d(in_channels=32, out_channels=64)
#output = involution(torch.rand(1, 32, 128, 128))

# Mount google drive to colab

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!ls -lrt /content/drive/MyDrive/CV_Project/Video

!ls -lrt





class Block_en(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding = 1)
        self.inonv1 = Involution2d(in_channels=in_ch, out_channels=out_ch, kernel_size = (3,3), padding = (1,1))
        self.relu  = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding = 1)
        self.inonv2 = Involution2d(in_channels=out_ch, out_channels=out_ch, kernel_size = (3,3), padding = (1,1))
    
    def forward(self, x):
        #return self.relu(self.conv2(self.relu(self.conv1(x))))
        #print(self.inonv1(x).shape)
        return self.relu(self.conv2(self.relu(self.inonv1(x))))
        #return self.relu(self.inonv2(self.relu(self.inonv1(x))))

class Block_de(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding = 1)
        #self.inonv1 = Involution2d(in_channels=in_ch, out_channels=out_ch, kernel_size = (3,3), padding = (1,1))
        self.relu  = nn.ReLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding = 1)
        #self.inonv2 = Involution2d(in_channels=out_ch, out_channels=out_ch, kernel_size = (3,3), padding = (1,1))
    
    def forward(self, x):
        return self.relu(self.conv2(self.relu(self.conv1(x))))
        #print(self.inonv1(x).shape)
        #return self.relu(self.conv2(self.relu(self.inonv1(x))))

class Encoder(nn.Module):
    def __init__(self, chs=(3,64,128,256,512,1024)):
        super().__init__()
        self.enc_blocks = nn.ModuleList([Block_en(chs[i], chs[i+1]) for i in range(len(chs)-1)])
        self.pool       = nn.MaxPool2d(2)
    
    def forward(self, x):
        ftrs = []
        for block in self.enc_blocks:
            x = block(x)
            ftrs.append(x)
            x = self.pool(x)
        return ftrs

class Decoder(nn.Module):
    def __init__(self, chs=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.chs         = chs
        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])
        self.dec_blocks = nn.ModuleList([Block_de(chs[i], chs[i+1]) for i in range(len(chs)-1)]) 
        
    def forward(self, x, encoder_features):
        for i in range(len(self.chs)-1):
            x        = self.upconvs[i](x)
            enc_ftrs = self.crop(encoder_features[i], x)
            x        = torch.cat([x, enc_ftrs], dim=1)
            x        = self.dec_blocks[i](x)
        return x
    
    def crop(self, enc_ftrs, x):
        _, _, H, W = x.shape
        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)
        return enc_ftrs

class InvolutionUNet(nn.Module):
    def __init__(self, enc_chs=(3,64,128,256,512), dec_chs=(512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):
        super().__init__()
        self.encoder     = Encoder(enc_chs)
        self.decoder     = Decoder(dec_chs)
        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)
        self.retain_dim  = retain_dim
        self.out_sz = out_sz

    def forward(self, x):
        enc_ftrs = self.encoder(x)
        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])
        out      = self.head(out)
        if self.retain_dim:
            out = F.interpolate(out, self.out_sz)
        return out





"""## Inference on the Best Model """



test_transform = A.Compose(
    [
        #A.CenterCrop(height=224, width=224),
        A.Resize(height=720, width=720),
        A.Normalize(mean=(0.64, 0.6, 0.58),std=(0.14,0.15, 0.152)),
        ToTensor(),
    ])



best_model = InvolutionUNet(enc_chs=(3,64,128,256), dec_chs=(256, 128, 64), num_class=3, retain_dim=False, out_sz=(128,128))

best_ckp = torch.load('/content/drive/MyDrive/CV_Project/CheckPoints/r3/unet-14.pt')

best_model.load_state_dict(best_ckp['model'])





def dehaze_video(video, model, transform, video_path = 'DeHazed.avi'):
  fps = video.get(cv2.CAP_PROP_FPS)
  out = cv2.VideoWriter(video_path,cv2.VideoWriter_fourcc('M','J','P','G'), fps, (720,720))
  frame_count = 0
  while(video.isOpened()):
    ret_val, new_frame = video.read()
    #print(new_frame.shape)
    mask = np.zeros_like(new_frame)
    frame_count = frame_count + 1
    if frame_count > video.get(cv2.CAP_PROP_FRAME_COUNT):
      return video_path
    if ret_val is True:
      new_frame = cv2.cvtColor(new_frame, cv2.COLOR_BGR2RGB)
      transformed = transform(image=new_frame, mask=mask)
      new_frame_transformed = transformed['image']
      #plt.imshow(new_frame_transformed.permute(1,2,0))
      #break
      model.eval()
      with torch.no_grad():
        images_fod = torch.tensor(new_frame_transformed).to(device).unsqueeze(0)
        model = model.to(device)
        pred_masks_fod = model(images_fod)
        #maxValue = np.amax(pred_masks_fod.detach().cpu().numpy())
        #minValue = np.amin(pred_masks_fod.detach().cpu().numpy())
        pred_masks_fod = (np.clip(pred_masks_fod.detach().cpu().squeeze(0).permute(1,2,0).numpy(), 0 , 1)*255).astype('uint8')
      '''
      if frame_count == 10:
        #print((pred_masks_fod.detach().cpu().squeeze(0).permute(1,2,0).numpy()))
        #plt.imshow(np.round_(pred_masks_fod.detach().cpu().squeeze(0).permute(1,2,0).numpy()))
        #plt.imshow((pred_masks_fod.detach().cpu().squeeze(0).permute(1,2,0).numpy()*255).astype('uint8'))
        print(maxValue)
        print(minValue)
        print("***********")
    
        print(pred_masks_fod)
        plt.imshow(pred_masks_fod)
        break
      '''
      #out.write((pred_masks_fod.detach().cpu().squeeze(0).permute(1,2,0).numpy()*255).astype('uint8'))
      out.write(pred_masks_fod)

video_path = '/content/drive/MyDrive/CV_Project/Video/Foggy_Forest.mov'
video = cv2.VideoCapture(video_path)

dehaze_video(video, best_model, test_transform, video_path = 'Defogged_Forest.avi')

!ffmpeg -i Defogged_Forest.avi Defogged_Forest.mp4

!cp ./Defogged_Forest.mp4 /content/drive/MyDrive/CV_Project/Video_Output/



"""### Foggy Video"""

from IPython.display import HTML
from base64 import b64encode
mp4 = open('/content/drive/MyDrive/CV_Project/Video/Foggy_Forest.mov','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)



"""### Dehazed Video"""

from IPython.display import HTML
from base64 import b64encode
mp4 = open('/content/drive/MyDrive/CV_Project/Video_Output/Defogged_Forest.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)

